{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "!pip -q install corus navec slovnet ipymarkup sklearn_crfsuite nerus nereval\n",
    "\n",
    "# DATASETS:\n",
    "# FactRuEval-2016\n",
    "!wget https://github.com/dialogue-evaluation/factRuEval-2016/archive/master.zip\n",
    "!unzip master.zip\n",
    "!rm master.zip\n",
    "# collection5 (ne5)\n",
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip\n",
    "!unzip collection5.zip\n",
    "!rm collection5.zip\n",
    "# BSNLP\n",
    "!wget http://bsnlp.cs.helsinki.fi/bsnlp-2019/TRAININGDATA_BSNLP_2019_shared_task.zip\n",
    "!wget http://bsnlp.cs.helsinki.fi/bsnlp-2019/TESTDATA_BSNLP_2019_shared_task.zip\n",
    "!unzip TRAININGDATA_BSNLP_2019_shared_task.zip\n",
    "!unzip TESTDATA_BSNLP_2019_shared_task.zip -d test_pl_cs_ru_bg\n",
    "!rm TRAININGDATA_BSNLP_2019_shared_task.zip TESTDATA_BSNLP_2019_shared_task.zip\n",
    "\n",
    "# for metrics computation\n",
    "!git clone https://github.com/davidsbatista/NER-Evaluation.git\n",
    "    \n",
    "# for slovnet\n",
    "!wget https://storage.yandexcloud.net/natasha-slovnet/packs/slovnet_ner_news_v1.tar\n",
    "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_news_v1_1B_250K_300d_100q.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T11:05:54.216308Z",
     "iopub.status.busy": "2023-11-18T11:05:54.215740Z",
     "iopub.status.idle": "2023-11-18T11:09:20.987133Z",
     "shell.execute_reply": "2023-11-18T11:09:20.985734Z",
     "shell.execute_reply.started": "2023-11-18T11:05:54.216254Z"
    }
   },
   "outputs": [],
   "source": [
    "# NERUS (не понадобился)\n",
    "!wget https://storage.yandexcloud.net/natasha-nerus/data/nerus_lenta.conllu.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-19T12:58:17.591727Z",
     "iopub.status.busy": "2023-11-19T12:58:17.591323Z",
     "iopub.status.idle": "2023-11-19T12:58:18.036603Z",
     "shell.execute_reply": "2023-11-19T12:58:18.035626Z",
     "shell.execute_reply.started": "2023-11-19T12:58:17.591687Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import corus\n",
    "from copy import deepcopy\n",
    "import nereval\n",
    "\n",
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from IPython.display import display\n",
    "\n",
    "import sys, os\n",
    "scriptpath = \"/kaggle/working/NER-Evaluation/ner_evaluation\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from ner_eval import compute_metrics, compute_precision_recall_wrapper, collect_named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:30:23.614334Z",
     "iopub.status.busy": "2023-11-18T17:30:23.614025Z",
     "iopub.status.idle": "2023-11-18T17:30:27.223498Z",
     "shell.execute_reply": "2023-11-18T17:30:27.222600Z",
     "shell.execute_reply.started": "2023-11-18T17:30:23.614309Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"WANDBKEY\"\n",
    "WANDB_KEY = UserSecretsClient().get_secret(secret_label)\n",
    "wandb.login(key=WANDB_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ознакомление с набором данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:08:31.000104Z",
     "iopub.status.busy": "2023-11-18T12:08:30.999659Z",
     "iopub.status.idle": "2023-11-18T12:08:31.014693Z",
     "shell.execute_reply": "2023-11-18T12:08:31.013551Z",
     "shell.execute_reply.started": "2023-11-18T12:08:31.000071Z"
    }
   },
   "outputs": [],
   "source": [
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:08:31.135388Z",
     "iopub.status.busy": "2023-11-18T12:08:31.134935Z",
     "iopub.status.idle": "2023-11-18T12:08:31.144011Z",
     "shell.execute_reply": "2023-11-18T12:08:31.142696Z",
     "shell.execute_reply.started": "2023-11-18T12:08:31.135356Z"
    }
   },
   "outputs": [],
   "source": [
    "for tagged_text in ne5:\n",
    "    print(tagged_text.text[:800])\n",
    "    print(tagged_text.spans[:18])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что тексты - новости, так же тут содержится спорный тег \"GEOPOLIT\", который можно, на мой взгляд, заменить на \"LOC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:12:19.742469Z",
     "iopub.status.busy": "2023-11-18T12:12:19.742073Z",
     "iopub.status.idle": "2023-11-18T12:12:19.919064Z",
     "shell.execute_reply": "2023-11-18T12:12:19.917882Z",
     "shell.execute_reply.started": "2023-11-18T12:12:19.742440Z"
    }
   },
   "outputs": [],
   "source": [
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "l = 0\n",
    "s = 0\n",
    "for tagged_text in ne5:\n",
    "    l += 1\n",
    "    s += len(tagged_text.text)\n",
    "print(\"Количество текстов: \", l)\n",
    "print(f\"Средняя длина текста: {s/l:.1f} символов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие датасеты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В планах изначально было дополнительно сравнить модели на датасетах FactRuEval-2016, но там используется более сложная система разметки: были выделены сущности, имеющие нормальные теги, хоть и написанные по-другому \"Loc\", \"Per\" и т. д. И внутри этих сущностей были выделены подсущности с названиями в духе \"loc_description\", \"name\", \"surname\", что потребовало бы больших затрат по времени на преобразование этих данных для сравнения с выводами моделей.\n",
    "\n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:19:36.570792Z",
     "iopub.status.busy": "2023-11-18T13:19:36.570394Z",
     "iopub.status.idle": "2023-11-18T13:19:36.579885Z",
     "shell.execute_reply": "2023-11-18T13:19:36.579107Z",
     "shell.execute_reply.started": "2023-11-18T13:19:36.570762Z"
    }
   },
   "outputs": [],
   "source": [
    "factru = corus.load_factru('/kaggle/working/factRuEval-2016-master')\n",
    "for tagged_text in factru:\n",
    "    text = tagged_text.text\n",
    "    print(text[:300])\n",
    "    f_objs = tagged_text.objects\n",
    "    print(tagged_text.objects[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:22:53.925051Z",
     "iopub.status.busy": "2023-11-18T13:22:53.924613Z",
     "iopub.status.idle": "2023-11-18T13:22:53.931626Z",
     "shell.execute_reply": "2023-11-18T13:22:53.930865Z",
     "shell.execute_reply.started": "2023-11-18T13:22:53.925019Z"
    }
   },
   "outputs": [],
   "source": [
    "for obj in f_objs[:5]:\n",
    "    print(\"Тип сущности:\", obj.type)\n",
    "    for span in obj.spans:\n",
    "        print(\"Тип подсущности:\", span.type)\n",
    "        print(\"Сама подсущность:\", text[span.start:span.stop])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний пример с org_descr вообще что-то очень странное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также был еще один вариант с BSNLP, но там разметка еще хуже: в сущностях не выделены их индексы в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:26:42.716696Z",
     "iopub.status.busy": "2023-11-18T13:26:42.716242Z",
     "iopub.status.idle": "2023-11-18T13:26:42.741876Z",
     "shell.execute_reply": "2023-11-18T13:26:42.740788Z",
     "shell.execute_reply.started": "2023-11-18T13:26:42.716662Z"
    }
   },
   "outputs": [],
   "source": [
    "bsnlp = corus.load_bsnlp('/kaggle/working/training_pl_cs_ru_bg_rc1')\n",
    "for tagged_text in bsnlp:\n",
    "    print(tagged_text.text[:200])\n",
    "    print(tagged_text.substrings[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому в итоге было решено тестировать модели на Collection5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Развертывание готовых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slovnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:08:28.421496Z",
     "iopub.status.busy": "2023-11-19T13:08:28.420783Z",
     "iopub.status.idle": "2023-11-19T13:08:29.119148Z",
     "shell.execute_reply": "2023-11-19T13:08:29.117840Z",
     "shell.execute_reply.started": "2023-11-19T13:08:28.421459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОАО СТАВРОПОЛЬЭНЕРГОБАНК В ЧЕРКЕССКЕ\n",
      "ORG─────────────────────   LOC──────\n"
     ]
    }
   ],
   "source": [
    "navec = Navec.load('/kaggle/working/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "slovnet = NER.load('/kaggle/working/slovnet_ner_news_v1.tar')\n",
    "slovnet.navec(navec);\n",
    "\n",
    "# Пример использования:\n",
    "markup = slovnet('ОАО СТАВРОПОЛЬЭНЕРГОБАНК В ЧЕРКЕССКЕ')\n",
    "\n",
    "show_markup(markup.text, markup.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заметка про метрики, предложенные SemEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда я читал про метрики, мне сразу попалась статья, где описывались более сложные метрики, чем те, что обычно используются в бенчмарках. Здесь есть 4 типа метрик:\n",
    "\n",
    "**Strict**: точное совпадение по позиции и по типу;\n",
    "\n",
    "**Exact**: точное совпадение по позиции несмотря на тип;\n",
    "\n",
    "**Partial**: частичное совпадение по позиции несмотря на тип;\n",
    "\n",
    "**Type**: совпадение по типу по частичному/полному совпадению по позиции.\n",
    "\n",
    "\n",
    "И по каждой из них результат на каком-либо объекте может быть следующим:\n",
    "\n",
    "*Correct (COR)* : совпадение;\n",
    "\n",
    "*Incorrect (INC)* : несовпадение;\n",
    "\n",
    "*Partial (PAR)* : предсказание модели и правильная разметка частично совпадают;\n",
    "\n",
    "*Missing (MIS)* : размеченная сущность не выделена моделью;\n",
    "\n",
    "*Spurius (SPU)* : модель выделила сущность, которой на самом деле нет.\n",
    "\n",
    "\n",
    "Тогда число возможных сущностей (POS) считается так:\n",
    "\n",
    "**POS** = COR + INC + PAR + MIS = TP + FN \n",
    "\n",
    "Число сущностей, выделенных моделью (ACT):\n",
    "\n",
    "**ACT** = COR + INC + PAR + SPU = TP + FP\n",
    "\n",
    "\n",
    "\n",
    "Для **Strict** и **Exact** (полное совпадение) precision и recall считаются по следующей формуле:\n",
    "\n",
    "Precision = COR / ACT = TP / (TP + FP)\n",
    "\n",
    "Recall = COR / POS = TP / (TP + FN)\n",
    "\n",
    "Для **Partial** и **Type** (частичное совпадение) используются следующие формулы:\n",
    "\n",
    "Precision = (COR + 0.5 * PAR) / ACT = TP / (TP + FP)\n",
    "\n",
    "Recall = (COR + 0.5 * PAR) / POS = TP / (TP + FN)\n",
    "\n",
    "Ссылка на источник: <a href=\"https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\">Named-Entity evaluation metrics based on entity-level</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попробую посчитать данные метрики для модели slovnet на датасете Collection5 для стандартного набора NER-токенов \"LOC\", \"PER\", \"ORG\", где токен \"GEOPOLIT\" будет заменен на \"LOC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:48:35.150979Z",
     "iopub.status.busy": "2023-11-18T12:48:35.150475Z",
     "iopub.status.idle": "2023-11-18T12:48:35.167470Z",
     "shell.execute_reply": "2023-11-18T12:48:35.166257Z",
     "shell.execute_reply.started": "2023-11-18T12:48:35.150922Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_f1(p, r):\n",
    "    if (p + r) == 0:\n",
    "        return 0\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def test_slovnet_ne5(track_scores):\n",
    "\n",
    "    metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
    "                       'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    results = {'strict': deepcopy(metrics_results),\n",
    "               'ent_type': deepcopy(metrics_results),\n",
    "               'partial':deepcopy(metrics_results),\n",
    "               'exact':deepcopy(metrics_results)\n",
    "              }\n",
    "\n",
    "    ent_types = ['PER', 'LOC', 'ORG']\n",
    "\n",
    "\n",
    "    # results aggregated by entity type\n",
    "    evaluation_agg_entities_type = {e: deepcopy(results) for e in ent_types}\n",
    "\n",
    "    for tagged_text in ne5:\n",
    "        markup = slovnet(tagged_text.text)\n",
    "\n",
    "        for span in tagged_text.spans:\n",
    "            span.e_type = span.type if span.type != 'GEOPOLIT' else 'LOC'\n",
    "            span.start_offset = span.start\n",
    "            span.end_offset = span.stop\n",
    "        for span in markup.spans:\n",
    "            span.e_type = span.type\n",
    "            span.start_offset = span.start\n",
    "            span.end_offset = span.stop\n",
    "            \n",
    "        \n",
    "\n",
    "        tmp_results, tmp_agg_results = compute_metrics(\n",
    "            tagged_text.spans, markup.spans,  ent_types\n",
    "        )\n",
    "\n",
    "        for eval_schema in results.keys():\n",
    "            for metric in metrics_results.keys():\n",
    "                results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "\n",
    "        results = compute_precision_recall_wrapper(results)\n",
    "\n",
    "        for e_type in ent_types:\n",
    "            for eval_schema in tmp_agg_results[e_type]:\n",
    "                for metric in tmp_agg_results[e_type][eval_schema]:\n",
    "                    evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
    "            # Calculate precision recall at the individual entity level\n",
    "            evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(evaluation_agg_entities_type[e_type])\n",
    "            \n",
    "        \n",
    "    for name, ress in results.items():\n",
    "        ress['f1'] = compute_f1(ress['precision'], ress['recall'])\n",
    "\n",
    "    for ent_name, ent_metrics in evaluation_agg_entities_type.items():\n",
    "        for name, ress in ent_metrics.items():\n",
    "            ress['f1'] = compute_f1(ress['precision'], ress['recall'])\n",
    "\n",
    "    return results, evaluation_agg_entities_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:48:48.965175Z",
     "iopub.status.busy": "2023-11-18T12:48:48.964160Z",
     "iopub.status.idle": "2023-11-18T12:49:44.306438Z",
     "shell.execute_reply": "2023-11-18T12:49:44.304813Z",
     "shell.execute_reply.started": "2023-11-18T12:48:48.965140Z"
    }
   },
   "outputs": [],
   "source": [
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "track_scores = ['precision', 'recall', 'f1']\n",
    "res, ev_agg = test_slovnet_ne5(track_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:52:43.045992Z",
     "iopub.status.busy": "2023-11-18T12:52:43.045527Z",
     "iopub.status.idle": "2023-11-18T12:52:43.055697Z",
     "shell.execute_reply": "2023-11-18T12:52:43.054630Z",
     "shell.execute_reply.started": "2023-11-18T12:52:43.045944Z"
    }
   },
   "outputs": [],
   "source": [
    "track_scores = ['precision', 'recall', 'f1']\n",
    "res_df = pd.DataFrame({\n",
    "    name: [res[name][score] for score in track_scores] for name in res.keys()\n",
    "}, index=track_scores)\n",
    "\n",
    "ent_dfs = []\n",
    "ent_types = ['PER', 'LOC', 'ORG']\n",
    "for ent_type in ent_types:\n",
    "    cur_dict = ev_agg[ent_type]\n",
    "    cur_df = pd.DataFrame({\n",
    "        name: [cur_dict[name][score] for score in track_scores] for name in cur_dict.keys()\n",
    "    }, index=track_scores)\n",
    "    ent_dfs.append(cur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:52:46.416476Z",
     "iopub.status.busy": "2023-11-18T12:52:46.416033Z",
     "iopub.status.idle": "2023-11-18T12:52:46.456832Z",
     "shell.execute_reply": "2023-11-18T12:52:46.455743Z",
     "shell.execute_reply.started": "2023-11-18T12:52:46.416442Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Result DF:')\n",
    "display(res_df)\n",
    "print('Person DF:')\n",
    "display(ent_dfs[0])\n",
    "print('Location DF:')\n",
    "display(ent_dfs[1])\n",
    "print('Organization DF:')\n",
    "display(ent_dfs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:02:47.701541Z",
     "iopub.status.busy": "2023-11-18T13:02:47.701103Z",
     "iopub.status.idle": "2023-11-18T13:02:47.708756Z",
     "shell.execute_reply": "2023-11-18T13:02:47.707600Z",
     "shell.execute_reply.started": "2023-11-18T13:02:47.701511Z"
    }
   },
   "outputs": [],
   "source": [
    "res[\"strict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти метрики очень хорошо отражают поведение модели для каждой сущности, однако довольно странно, что strict и exact оказались нулевыми - результаты, полученные с помощью compute_metrics из гитхаба <a href=\"https://github.com/davidsbatista/NER-Evaluation\">NER-Evaluation</a> (автора статьи, приведенной в начале), показывают, что все точные совпадения по типу и позиции имеют тип \"incorrect\", скорее всего проблема где-то в реализации этого метода. \n",
    "\n",
    "Однако, в связи вышеупомянутыми странностями в полученных результатах и сложности сравнения по данным метрикам, я решил для сравнения моделей использовать общепринятую метрику F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nereval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь метрики считаются по следующим формулам:\n",
    "\n",
    "Precision = COR/ACT\n",
    "\n",
    "Recall = COR/POS\n",
    "\n",
    "F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "Правильные ответы модели - совпадение по тексту или типу, более подробно можно посмотреть в реализации мини-библиотеки <a href=\"https://github.com/jantrienes/nereval/tree/master\">Nereval</a>, а именно в реализации <a href=\"https://github.com/jantrienes/nereval/blob/master/nereval.py\">nereval.py</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:08:43.914188Z",
     "iopub.status.busy": "2023-11-19T13:08:43.913825Z",
     "iopub.status.idle": "2023-11-19T13:09:23.807882Z",
     "shell.execute_reply": "2023-11-19T13:09:23.806551Z",
     "shell.execute_reply.started": "2023-11-19T13:08:43.914155Z"
    }
   },
   "outputs": [],
   "source": [
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "y_true = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "y_pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "\n",
    "for tagged_text in ne5:\n",
    "    markup = slovnet(tagged_text.text)\n",
    "    corr = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    \n",
    "    # Ground-truth сущности\n",
    "    for span in tagged_text.spans:\n",
    "        if span.type in y_pred.keys() or span.type == 'GEOPOLIT':\n",
    "            ec = nereval.Entity(\n",
    "                span.text,\n",
    "                span.type if span.type != 'GEOPOLIT' else 'LOC',\n",
    "                span.start\n",
    "            )\n",
    "            corr[ec.type].append(ec)\n",
    "            corr[\"overall\"].append(ec)\n",
    "    \n",
    "    # Сущности, найденные моделью\n",
    "    for span in markup.spans:\n",
    "        if span.type in y_pred.keys():\n",
    "            ep = nereval.Entity(\n",
    "                markup.text[span.start:span.stop],\n",
    "                span.type,\n",
    "                span.start\n",
    "            )\n",
    "            pred[ep.type].append(ep)\n",
    "            pred[\"overall\"].append(ep)\n",
    "    \n",
    "    for key in y_true.keys():\n",
    "        y_true[key].append(corr[key])\n",
    "        y_pred[key].append(pred[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:13:04.207462Z",
     "iopub.status.busy": "2023-11-19T13:13:04.206695Z",
     "iopub.status.idle": "2023-11-19T13:13:05.085255Z",
     "shell.execute_reply": "2023-11-19T13:13:05.084481Z",
     "shell.execute_reply.started": "2023-11-19T13:13:04.207428Z"
    }
   },
   "outputs": [],
   "source": [
    "true_slovnet = deepcopy(y_true)\n",
    "pred_slovnet = deepcopy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:09:43.404515Z",
     "iopub.status.busy": "2023-11-19T13:09:43.403644Z",
     "iopub.status.idle": "2023-11-19T13:09:44.075068Z",
     "shell.execute_reply": "2023-11-19T13:09:44.074178Z",
     "shell.execute_reply.started": "2023-11-19T13:09:43.404479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL F1 для Slovnet: 0.9420835854691678\n",
      "LOC F1 для Slovnet: 0.9777503090234858\n",
      "PER F1 для Slovnet: 0.9791983282674771\n",
      "ORG F1 для Slovnet: 0.8513227852155532\n"
     ]
    }
   ],
   "source": [
    "for metric in y_true.keys():\n",
    "    print(f\"{metric.upper()} F1 для Slovnet: {nereval.evaluate(y_true[metric], y_pred[metric])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и настройка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T12:59:12.818136Z",
     "iopub.status.busy": "2023-11-19T12:59:12.817313Z",
     "iopub.status.idle": "2023-11-19T12:59:25.789620Z",
     "shell.execute_reply": "2023-11-19T12:59:25.788346Z",
     "shell.execute_reply.started": "2023-11-19T12:59:12.818104Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q naeval intervaltree razdel requests docker pullenti_client razdel pymorphy2 gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T12:59:25.791870Z",
     "iopub.status.busy": "2023-11-19T12:59:25.791538Z",
     "iopub.status.idle": "2023-11-19T13:00:11.174060Z",
     "shell.execute_reply": "2023-11-19T13:00:11.173068Z",
     "shell.execute_reply.started": "2023-11-19T12:59:25.791841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.10.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\n",
      "chex 0.1.84 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n",
      "fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
      "fitter 1.6.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.64.1 which is incompatible.\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n",
      "tensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\n",
      "virtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобство использование модели от Deeppavlov заключается в том, что можно менять их конфиг так, как удобно. Так что я добавлю в вывод модели, в котором уже есть токены и предсказания, позиции токенов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:45:09.928422Z",
     "iopub.status.busy": "2023-11-19T13:45:09.927615Z",
     "iopub.status.idle": "2023-11-19T13:45:09.939359Z",
     "shell.execute_reply": "2023-11-19T13:45:09.938311Z",
     "shell.execute_reply.started": "2023-11-19T13:45:09.928379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_tokens', 'y_pred', 'tokens_offsets']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "cfg = parse_config('ner_collection3_bert')\n",
    "cfg['chainer']['out'] += ['tokens_offsets']\n",
    "cfg['chainer']['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:45:10.047597Z",
     "iopub.status.busy": "2023-11-19T13:45:10.046927Z",
     "iopub.status.idle": "2023-11-19T13:45:10.057107Z",
     "shell.execute_reply": "2023-11-19T13:45:10.056142Z",
     "shell.execute_reply.started": "2023-11-19T13:45:10.047557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_tokens', 'y_pred', 'tokens_offsets']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg2 = parse_config('ner_rus_bert')\n",
    "cfg2['chainer']['out'] += ['tokens_offsets']\n",
    "cfg2['chainer']['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:45:17.609890Z",
     "iopub.status.busy": "2023-11-19T13:45:17.609106Z",
     "iopub.status.idle": "2023-11-19T13:46:32.810116Z",
     "shell.execute_reply": "2023-11-19T13:46:32.809348Z",
     "shell.execute_reply.started": "2023-11-19T13:45:17.609852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch<1.14.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (68.1.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.41.2)\n",
      "Requirement already satisfied: pytorch-crf==0.7.* in /opt/conda/lib/python3.10/site-packages (0.7.2)\n",
      "Ignoring transformers: markers 'python_version < \"3.8\"' don't match your environment\n",
      "Requirement already satisfied: transformers==4.30.0 in /opt/conda/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 13:45:59.284 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz to /root/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz\n",
      "100%|██████████| 1.44G/1.44G [00:13<00:00, 109MB/s] \n",
      "2023-11-19 13:46:12.659 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting /root/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz archive into /root/.deeppavlov/models/ner_rus_bert_torch\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner_model2 = build_model(cfg2, download=True, install=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее можно просто загрузить готовую для использования модель, используя кастомный конфиг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:00:11.277840Z",
     "iopub.status.busy": "2023-11-19T13:00:11.277184Z",
     "iopub.status.idle": "2023-11-19T13:03:36.573500Z",
     "shell.execute_reply": "2023-11-19T13:03:36.572484Z",
     "shell.execute_reply.started": "2023-11-19T13:00:11.277804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch<1.14.0,>=1.6.0\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (4.5.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<1.14.0,>=1.6.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<1.14.0,>=1.6.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<1.14.0,>=1.6.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<1.14.0,>=1.6.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (68.1.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.41.2)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
      "Collecting pytorch-crf==0.7.*\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n",
      "Ignoring transformers: markers 'python_version < \"3.8\"' don't match your environment\n",
      "Collecting transformers==4.30.0\n",
      "  Obtaining dependency information for transformers==4.30.0 from https://files.pythonhosted.org/packages/e2/72/1af3d38e98fdcceb3876de4567ac395a66c26976e259fe2d46266e052d61/transformers-4.30.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2023.7.22)\n",
      "Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.0\n",
      "    Uninstalling transformers-4.35.0:\n",
      "      Successfully uninstalled transformers-4.35.0\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 13:02:31.591 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_coll3_torch.tar.gz to /root/.deeppavlov/models/ner_rus_bert_coll3_torch.tar.gz\n",
      "100%|██████████| 1.44G/1.44G [00:21<00:00, 67.2MB/s]\n",
      "2023-11-19 13:02:53.140 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting /root/.deeppavlov/models/ner_rus_bert_coll3_torch.tar.gz archive into /root/.deeppavlov/models/ner_rus_bert_coll3_torch\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d28d510e04ff3819c39255fa89b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8954f9a8654825ae45b7af364adb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b07e8f7d94341aa3ef19587252eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bdcd158c75403c9816fbcf752a58d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62442c9c2acf4c088effb3b5c302ea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model(cfg, download=True, install=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T14:09:20.055805Z",
     "iopub.status.busy": "2023-11-18T14:09:20.055403Z",
     "iopub.status.idle": "2023-11-18T14:09:20.166845Z",
     "shell.execute_reply": "2023-11-18T14:09:20.165507Z",
     "shell.execute_reply.started": "2023-11-18T14:09:20.055775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Пример работы:\n",
    "ex_dp = ner_model(['ОАО СТАВРОПОЛЬЭНЕРГОБАНК В ЧЕРКЕССКЕ'])\n",
    "print(\"Разделение на токены:\", ex_dp[0][0])\n",
    "print(\"NER-токены:\", ex_dp[1][0])\n",
    "print(\"Позиции токенов в тексте:\", ex_dp[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение результатов на датасете Collection5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В документации написано, что есть три типа тегов B - начало слова, E - конец, I - то, что внутри. Однако, просмотрев результат работы, очевидно, что есть еще один S - единичный NER-токен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также эта модель основана на трансформере BERT и может принимать тексты, после токенизации которых получается не больше 512 токенов. Поэтому придется токенизировать текст токенизатором изначальной модели, разбить на батчи по 512 токенов и потом итерироваться по каждому батчу, превращая токены обратно в текст тем же токенизатором.\n",
    "\n",
    "UPD: попробовал этот подход и понял, что токенизируют они текст всё равно по-разному, поэтому решил просто разбивать по пробелу на батчи по 256 разбиений, чтобы точно не было конфликтов. К тому же, полученный обратно текст со сплита точно будет таким же, в отличие от tokenizer.convert_tokens_to_string(), который не гарантирует, что тексты будут одинаковые.\n",
    "\n",
    "UPD2: вместо разделения с помощью сплит теги \\r \\n \\t превращаются в пробелы, из-за этого слетают позиции слов в предсказаниях и падают метрики. Решение - использовать регулярные выражения для разделения текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T14:45:17.696033Z",
     "iopub.status.busy": "2023-11-18T14:45:17.694902Z",
     "iopub.status.idle": "2023-11-18T14:45:17.703555Z",
     "shell.execute_reply": "2023-11-18T14:45:17.702282Z",
     "shell.execute_reply.started": "2023-11-18T14:45:17.695991Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tkn = cfg['chainer']['pipe'][2]['pretrained_bert']\n",
    "model_tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T14:46:03.670462Z",
     "iopub.status.busy": "2023-11-18T14:46:03.670057Z",
     "iopub.status.idle": "2023-11-18T14:46:04.148681Z",
     "shell.execute_reply": "2023-11-18T14:46:04.147681Z",
     "shell.execute_reply.started": "2023-11-18T14:46:03.670431Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:38:28.352755Z",
     "iopub.status.busy": "2023-11-19T13:38:28.352406Z",
     "iopub.status.idle": "2023-11-19T13:38:28.360900Z",
     "shell.execute_reply": "2023-11-19T13:38:28.360055Z",
     "shell.execute_reply.started": "2023-11-19T13:38:28.352729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpanMarkup(\n",
       "    text='Ольга Иванова-Степанова',\n",
       "    spans=[Span(\n",
       "         start=0,\n",
       "         stop=23,\n",
       "         type='PER'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slovnet('Ольга Иванова-Степанова')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:38:28.494925Z",
     "iopub.status.busy": "2023-11-19T13:38:28.494344Z",
     "iopub.status.idle": "2023-11-19T13:38:28.513010Z",
     "shell.execute_reply": "2023-11-19T13:38:28.512172Z",
     "shell.execute_reply.started": "2023-11-19T13:38:28.494896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Ольга', 'Иванова', '-', 'Степанова']],\n",
       " [['B-PER', 'E-PER', 'E-PER', 'E-PER']],\n",
       " [[(0, 5), (6, 13), (13, 14), (14, 23)]]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['Ольга Иванова-Степанова'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:47:22.134866Z",
     "iopub.status.busy": "2023-11-19T13:47:22.134008Z",
     "iopub.status.idle": "2023-11-19T13:47:22.154027Z",
     "shell.execute_reply": "2023-11-19T13:47:22.153121Z",
     "shell.execute_reply.started": "2023-11-19T13:47:22.134832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Ольга', 'Иванова', '-', 'Степанова']],\n",
       " [['B-PER', 'I-PER', 'I-PER', 'I-PER']],\n",
       " [[(0, 5), (6, 13), (13, 14), (14, 23)]]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model2(['Ольга Иванова-Степанова'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:38:45.827769Z",
     "iopub.status.busy": "2023-11-19T13:38:45.826716Z",
     "iopub.status.idle": "2023-11-19T13:38:45.838463Z",
     "shell.execute_reply": "2023-11-19T13:38:45.837254Z",
     "shell.execute_reply.started": "2023-11-19T13:38:45.827730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpanMarkup(\n",
       "    text='Би-би-си',\n",
       "    spans=[Span(\n",
       "         start=0,\n",
       "         stop=8,\n",
       "         type='ORG'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slovnet('Би-би-си')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:40:11.161457Z",
     "iopub.status.busy": "2023-11-19T13:40:11.160608Z",
     "iopub.status.idle": "2023-11-19T13:40:11.181382Z",
     "shell.execute_reply": "2023-11-19T13:40:11.180376Z",
     "shell.execute_reply.started": "2023-11-19T13:40:11.161413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Би', '-', 'би', '-', 'си']],\n",
       " [['S-ORG', 'S-ORG', 'S-ORG', 'S-ORG', 'S-ORG']],\n",
       " [[(0, 2), (2, 3), (3, 5), (5, 6), (6, 8)]]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['Би-би-си'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:46:45.637283Z",
     "iopub.status.busy": "2023-11-19T13:46:45.636406Z",
     "iopub.status.idle": "2023-11-19T13:46:45.656196Z",
     "shell.execute_reply": "2023-11-19T13:46:45.655370Z",
     "shell.execute_reply.started": "2023-11-19T13:46:45.637246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Би', '-', 'би', '-', 'си']],\n",
       " [['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG']],\n",
       " [[(0, 2), (2, 3), (3, 5), (5, 6), (6, 8)]]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model2(['Би-би-си'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:11:04.123563Z",
     "iopub.status.busy": "2023-11-19T15:11:04.122909Z",
     "iopub.status.idle": "2023-11-19T15:11:04.143843Z",
     "shell.execute_reply": "2023-11-19T15:11:04.142937Z",
     "shell.execute_reply.started": "2023-11-19T15:11:04.123528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ОАО', '\"', 'Би', '-', 'би', '-', 'си', '\"', '', '.']],\n",
       " [['B-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'O',\n",
       "   'O']],\n",
       " [[(0, 3),\n",
       "   (3, 4),\n",
       "   (4, 6),\n",
       "   (6, 7),\n",
       "   (7, 9),\n",
       "   (9, 10),\n",
       "   (10, 12),\n",
       "   (12, 13),\n",
       "   (13, 13),\n",
       "   (13, 14)]]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model2(['ОАО \"Би-би-си\".'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:12:34.352861Z",
     "iopub.status.busy": "2023-11-19T15:12:34.351985Z",
     "iopub.status.idle": "2023-11-19T15:12:34.358166Z",
     "shell.execute_reply": "2023-11-19T15:12:34.357294Z",
     "shell.execute_reply.started": "2023-11-19T15:12:34.352828Z"
    }
   },
   "outputs": [],
   "source": [
    "class TempEnt:\n",
    "    def __init__(self, start = None, stop = None, type_ = None):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.type = type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:41:08.007199Z",
     "iopub.status.busy": "2023-11-19T15:41:08.006834Z",
     "iopub.status.idle": "2023-11-19T15:41:47.147330Z",
     "shell.execute_reply": "2023-11-19T15:41:47.146356Z",
     "shell.execute_reply.started": "2023-11-19T15:41:08.007166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity(text='ОАО \" Современный коммерческий флот \" (ОАО \" Совкомфлот \")', type='ORG', start=206)\n",
      "Entity(text='ОАО \" Новороссийское морское пароходство \" (ОАО \" Новошип \")', type='ORG', start=616)\n",
      "Entity(text='ОАО \" НК \" Роснефть \"', type='ORG', start=112)\n",
      "Entity(text='ОАО \" НК \" Роснефть \"', type='ORG', start=421)\n",
      "Entity(text='', type='PER', start=1753)\n",
      "Entity(text='ОАО \" Акционерный коммерческий банк \" Банк Москвы \"', type='ORG', start=3264)\n",
      "Entity(text='АО \" Нефтяная компания \" ЮКОС \"', type='ORG', start=1941)\n",
      "Entity(text='ОАО \" ОК \" Русал \"', type='ORG', start=1349)\n",
      "Entity(text='', type='ORG', start=1395)\n",
      "Entity(text='Единая Россия Ватаным Татарстан \" (\" Моя родина Татарстан \"', type='ORG', start=150)\n",
      "Entity(text='ОАО \" НПО \" Сатурн \"', type='ORG', start=2549)\n",
      "Entity(text='\" Ата-Журт \" (\" Отечество \")', type='ORG', start=1122)\n",
      "Entity(text='\" Ар-Намыс \" (\" Достоинство \")', type='ORG', start=1281)\n",
      "Entity(text='Леонид Кожара. Укрзалізниці \" (\" Украинских железных дорог \"', type='PER', start=1326)\n",
      "Entity(text='ЗАО \" ПО \" Сибтяжмаш \"', type='ORG', start=1446)\n",
      "Entity(text='ЗАО \" ПО \" Сибтяжмаш \"', type='ORG', start=2000)\n",
      "Entity(text='ЗАО \" ПО \" Сибтяжмаш \"', type='ORG', start=2321)\n",
      "Entity(text='ОАО \" ВПК \" НПО машиностроения \"', type='ORG', start=607)\n",
      "Entity(text='', type='ORG', start=1217)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "\n",
    "ent_types = ['LOC', 'PER', 'ORG']\n",
    "\n",
    "y_true = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "y_pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "\n",
    "# знаки, до и после которых не надо ставить пробел\n",
    "#no_space = ['-', '(', ')']\n",
    "no_space_before = ['-', ')', '>', '»', '/']\n",
    "no_space_after = ['-', '(', '<', '«', '/']\n",
    "#to_delete = ['\"', \"'\", '(', ')', '<', '>', '.', ',', ':', ';', '«', '»']\n",
    "# пунктуационные символы, которые не нужно добавлять в предсказания\n",
    "end_punct = ['.', ':', ',', ';']\n",
    "\n",
    "# def get_clean_text(text):\n",
    "#     for s in to_delete:\n",
    "#         text = text.replace(s, '')\n",
    "#     return text\n",
    "\n",
    "for tagged_text in ne5:\n",
    "    \n",
    "    corr = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    \n",
    "    \n",
    "    cur_text = tagged_text.text\n",
    "    #tokenized_text = tokenizer.tokenize(cur_text)\n",
    "    batch_size=512\n",
    "    ##splitted_text = cur_text.split()\n",
    "    \n",
    "    splitted_text = re.split(r'(\\s+)', cur_text) # разбиение текста по пробелам\n",
    "    #token_batches = [tokenized_text[i:i + batch_size] for i in range(0, len(tokenized_text), batch_size)]\n",
    "    token_batches = [splitted_text[i:i + batch_size] for i in range(0, len(splitted_text), batch_size)]\n",
    "    \n",
    "    spans_pred = []\n",
    "    prev_text_len = 0 # предыдущая длина текста для корректной расстановки стартовых позиций\n",
    "\n",
    "    for batch in token_batches:\n",
    "        #input_text = tokenizer.convert_tokens_to_string(batch)\n",
    "        input_text = ''.join(batch)\n",
    "        tokens, ner_tokens, offsets = ner_model2([input_text])\n",
    "        ner_tokens = ner_tokens[0]\n",
    "        offsets = offsets[0]\n",
    "#         for tkn, ntkn in zip(tokens[0], ner_tokens):\n",
    "#             print(f\"{tkn} -- {ntkn}\")\n",
    "#         break\n",
    "        for i in range(len(ner_tokens)):\n",
    "            token = ner_tokens[i]\n",
    "            if len(token.split('-')) > 1 and token.split('-')[1] in ent_types: # если тег не O\n",
    "                if token[0] in ['S', 'B']: # начала сущностей или single сущности\n",
    "                    new_text = input_text[offsets[i][0]:offsets[i][1]]\n",
    "#                     if len(new_text) > 0 and new_text[-1] in end_punct: # удаляем пунктуацию из конца строки\n",
    "#                         new_text = new_text[:-1]\n",
    "                    spans_pred.append(nereval.Entity(\n",
    "                        #get_clean_text(new_text),\n",
    "                        new_text,\n",
    "                        token.split('-')[1],\n",
    "                        prev_text_len + offsets[i][0]\n",
    "                    ))\n",
    "                elif token[0] in ['I', 'E']: # токены, входящие в уже существующие сущности\n",
    "                    if len(spans_pred) > 0:\n",
    "                        old_text = spans_pred[-1].text\n",
    "                        new_text = input_text[offsets[i][0]:offsets[i][1]]\n",
    "                        if len(new_text) > 0:\n",
    "                            # пробел/не пробел для разных знаков\n",
    "                            if (new_text[0] in no_space_before or old_text[-1] in no_space_after):\n",
    "                                r = '' # r - разделитель\n",
    "                            else:\n",
    "                                r = ' '\n",
    "                            # удаление пунктуации из конца строки\n",
    "#                             if new_text[-1] in end_punct:\n",
    "#                                 new_text = new_text[:-1]\n",
    "                        spans_pred[-1] = nereval.Entity(\n",
    "                            #get_clean_text(old_text + r + new_text),\n",
    "                            old_text + r + new_text,\n",
    "                            spans_pred[-1].type,\n",
    "                            spans_pred[-1].start\n",
    "                        )\n",
    "                    # Почему-то модель может генерерировать E теги вместо S\n",
    "                    else:\n",
    "                        spans_pred.append(nereval.Entity(\n",
    "                            input_text[offsets[i][0]:offsets[i][1]],\n",
    "                            token.split('-')[1],\n",
    "                            prev_text_len + offsets[i][0]\n",
    "                        ))\n",
    "                elif token[0] == 'O':\n",
    "                    pass\n",
    "                else:\n",
    "                    print('Unexpected token:', token)\n",
    "        prev_text_len += len(input_text)\n",
    "    \n",
    "\n",
    "    \n",
    "    spans_true = []\n",
    "    for span in tagged_text.spans:\n",
    "        t = span.type if span.type != 'GEOPOLIT' else 'LOC'\n",
    "        if t in corr.keys():\n",
    "            text = span.text\n",
    "            for symbol in end_punct:\n",
    "                if text[-1] == symbol:\n",
    "                    text = text[:-1]\n",
    "            spans_true.append(nereval.Entity(\n",
    "                text,\n",
    "                t,\n",
    "                span.start \n",
    "            ))\n",
    "    \n",
    "    for span_true in spans_true:\n",
    "        corr[span_true.type].append(span_true)\n",
    "    corr[\"overall\"] = spans_true\n",
    "    \n",
    "    new_spans_pred = []\n",
    "    for span_pred in spans_pred:\n",
    "        \n",
    "        text = span_pred.text\n",
    "        if len(text) > 0:\n",
    "            if text[-1] in end_punct:\n",
    "                text = text[:-1]\n",
    "            list_double = [m.start() for m in re.finditer('\"', text)]\n",
    "            list_single = [m.start() for m in re.finditer(\"'\", text)]\n",
    "            for l in [list_double, list_single]:\n",
    "                if len(l) == 2:\n",
    "                    text = text[:l[0] + 1] + text[l[0] + 2:l[1] - 1] + text[l[1]:]\n",
    "                elif len(l) == 1:\n",
    "                    text = text[:l[0] + 1] + text[l[0] + 2:]\n",
    "                elif len(l) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(span_pred)\n",
    "        else:\n",
    "            print(span_pred)\n",
    "        new_spans_pred.append(nereval.Entity(\n",
    "            text,\n",
    "            span_pred.type,\n",
    "            span_pred.start\n",
    "        ))\n",
    "    for span_pred in new_spans_pred:\n",
    "        pred[span_pred.type].append(span_pred)\n",
    "    pred[\"overall\"] = new_spans_pred\n",
    "    \n",
    "    for key in y_true.keys():\n",
    "        y_true[key].append(corr[key])\n",
    "        y_pred[key].append(pred[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:38:12.422072Z",
     "iopub.status.busy": "2023-11-19T15:38:12.421362Z",
     "iopub.status.idle": "2023-11-19T15:38:50.609981Z",
     "shell.execute_reply": "2023-11-19T15:38:50.608995Z",
     "shell.execute_reply.started": "2023-11-19T15:38:12.422029Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "\n",
    "ent_types = ['LOC', 'PER', 'ORG']\n",
    "\n",
    "y_true = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "y_pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "\n",
    "# знаки, до и после которых не надо ставить пробел\n",
    "no_space_before = ['-', ')', '>', '»']\n",
    "no_space_after = ['-', '(', '<', '«']\n",
    "# пунктуационные символы, которые не нужно добавлять в предсказания\n",
    "end_punct = ['.', ':', ',', ';', ')', '>', '»', '(', '<', '«', \"'\", '\"', ' ']\n",
    "\n",
    "\n",
    "for tagged_text in ne5:\n",
    "    \n",
    "    corr = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    pred = {\"overall\": [], \"LOC\": [], \"PER\": [], \"ORG\": []}\n",
    "    \n",
    "    \n",
    "    cur_text = tagged_text.text\n",
    "    #tokenized_text = tokenizer.tokenize(cur_text)\n",
    "    batch_size=512\n",
    "    ##splitted_text = cur_text.split()\n",
    "    \n",
    "    splitted_text = re.split(r'(\\s+)', cur_text) # разбиение текста по пробелам\n",
    "    #token_batches = [tokenized_text[i:i + batch_size] for i in range(0, len(tokenized_text), batch_size)]\n",
    "    token_batches = [splitted_text[i:i + batch_size] for i in range(0, len(splitted_text), batch_size)]\n",
    "    \n",
    "    spans_pred = []\n",
    "    prev_text_len = 0 # предыдущая длина текста для корректной расстановки стартовых позиций\n",
    "\n",
    "    for batch in token_batches:\n",
    "        #input_text = tokenizer.convert_tokens_to_string(batch)\n",
    "        input_text = ''.join(batch)\n",
    "        tokens, ner_tokens, offsets = ner_model2([input_text])\n",
    "        ner_tokens = ner_tokens[0]\n",
    "        offsets = offsets[0]\n",
    "#         for tkn, ntkn in zip(tokens[0], ner_tokens):\n",
    "#             print(f\"{tkn} -- {ntkn}\")\n",
    "#         break\n",
    "        for i in range(len(ner_tokens)):\n",
    "            token = ner_tokens[i]\n",
    "            if len(token.split('-')) > 1 and token.split('-')[1] in ent_types: # если тег не O\n",
    "                if token[0] in ['S', 'B']: # начала сущностей или single сущности\n",
    "#                     new_text = input_text[offsets[i][0]:offsets[i][1]]\n",
    "#                     spans_pred.append(nereval.Entity(\n",
    "#                         new_text,\n",
    "#                         token.split('-')[1],\n",
    "#                         prev_text_len + offsets[i][0]\n",
    "#                     ))\n",
    "                    spans_pred.append(TempEnt(\n",
    "                        offsets[i][0],\n",
    "                        offsets[i][1],\n",
    "                        token.split('-')[1],\n",
    "                    ))\n",
    "                elif token[0] in ['I', 'E']: # токены, входящие в уже существующие сущности\n",
    "                    if len(spans_pred) > 0:\n",
    "#                         old_text = spans_pred[-1].text\n",
    "#                         new_text = input_text[offsets[i][0]:offsets[i][1]]\n",
    "\n",
    "#                         spans_pred[-1] = nereval.Entity(\n",
    "#                             old_text + r + new_text,\n",
    "#                             spans_pred[-1].type,\n",
    "#                             spans_pred[-1].start\n",
    "#                         )\n",
    "                        spans_pred[-1] = TempEnt(\n",
    "                            spans_pred[-1].start,\n",
    "                            offsets[i][1],\n",
    "                            spans_pred[-1].type,\n",
    "                        )\n",
    "                    # Почему-то модель может генерерировать E теги вместо S\n",
    "                    else:\n",
    "                        spans_pred.append(TempEnt(\n",
    "                            offsets[i][0],\n",
    "                            offsets[i][1],\n",
    "                            token.split('-')[1]\n",
    "                        ))\n",
    "#                         spans_pred.append(nereval.Entity(\n",
    "#                             input_text[offsets[i][0]:offsets[i][1]],\n",
    "#                             token.split('-')[1],\n",
    "#                             prev_text_len + offsets[i][0]\n",
    "#                         ))\n",
    "                        \n",
    "                elif token[0] == 'O':\n",
    "                    pass\n",
    "                else:\n",
    "                    print('Unexpected token:', token)\n",
    "        prev_text_len += len(input_text)\n",
    "    \n",
    "\n",
    "    \n",
    "    spans_true = []\n",
    "    for span in tagged_text.spans:\n",
    "        t = span.type if span.type != 'GEOPOLIT' else 'LOC'\n",
    "        if t in corr.keys():\n",
    "            text = span.text\n",
    "            while text[-1] in end_punct:\n",
    "                text = text[:-1]\n",
    "            spans_true.append(nereval.Entity(\n",
    "                text,\n",
    "                t,\n",
    "                span.start \n",
    "            ))\n",
    "    \n",
    "    for span_true in spans_true:\n",
    "        corr[span_true.type].append(span_true)\n",
    "    corr[\"overall\"] = spans_true\n",
    "    \n",
    "    new_spans_pred = []\n",
    "    for span_pred in spans_pred:\n",
    "        text = cur_text[span_pred.start:span_pred.stop]\n",
    "        while len(text) > 0 and text[-1] in end_punct:\n",
    "            text = text[:-1]\n",
    "        else:\n",
    "            #print(text)\n",
    "            pass\n",
    "        new_spans_pred.append(nereval.Entity(\n",
    "            text,\n",
    "            span_pred.type,\n",
    "            span_pred.start\n",
    "        ))\n",
    "    for span_pred in new_spans_pred:\n",
    "        pred[span_pred.type].append(span_pred)\n",
    "    pred[\"overall\"] = new_spans_pred\n",
    "    \n",
    "    for key in y_true.keys():\n",
    "        y_true[key].append(corr[key])\n",
    "        y_pred[key].append(pred[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:54:37.962308Z",
     "iopub.status.busy": "2023-11-19T15:54:37.961432Z",
     "iopub.status.idle": "2023-11-19T15:54:38.017246Z",
     "shell.execute_reply": "2023-11-19T15:54:38.016245Z",
     "shell.execute_reply.started": "2023-11-19T15:54:37.962260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity(text='РФ', type='LOC', start=362) - Entity(text='РФ-', type='LOC', start=362)\n",
      "Entity(text='Л.Швецова', type='PER', start=1128) - Entity(text='Л.Швецова политикой', type='PER', start=1128)\n",
      "Entity(text='СК', type='ORG', start=1636) - Entity(text='СК РФ', type='ORG', start=1636)\n",
      "Entity(text='ОАО \"Аэрофлот - российские авиалинии\"', type='ORG', start=157) - Entity(text='ОАО \"Аэрофлот-российские авиалинии\"', type='ORG', start=157)\n",
      "Entity(text='ФГАОУ ВПО \"Национальный исследовательский университет \"Высшая школа экономики\"', type='ORG', start=517) - Entity(text='ФГАОУ ВПО \"Национальный исследовательский университет\" Высшая школа экономики', type='ORG', start=517)\n",
      "Entity(text='ЗАО \"Национальная резервная корпорация\"', type='ORG', start=726) - Entity(text='ЗАО \"Национальная резервная корпорация', type='ORG', start=726)\n",
      "Entity(text='Аэрофлота', type='ORG', start=1328) - Entity(text='Аэрофлота \"по и', type='ORG', start=1328)\n",
      "Entity(text='Федерального собрания', type='ORG', start=1662) - Entity(text='Федерального собрания РФ', type='ORG', start=1662)\n",
      "Entity(text='ЗАО \"Русская рыбная компания\"', type='ORG', start=1081) - Entity(text='ЗАО \"Русская рыбная компания', type='ORG', start=1081)\n",
      "Entity(text='Галина Хованская', type='PER', start=1307) - Entity(text='Галина Хованская Тарнавский', type='PER', start=1307)\n",
      "Entity(text='Общенационального совета партии', type='ORG', start=1571) - Entity(text='Общенационального совета', type='ORG', start=1571)\n",
      "Entity(text='Центрального совета', type='ORG', start=2040) - Entity(text='Центрального совета партии', type='ORG', start=2040)\n",
      "Entity(text='Российского научного центра \"Курчатовский институт\"', type='ORG', start=677) - Entity(text='Российского научного центра \"Курчатовский институт', type='ORG', start=677)\n",
      "Entity(text='Достижения молодых', type='ORG', start=797) - Entity(text='Достижения молодых палате', type='ORG', start=797)\n",
      "Entity(text='Департамента информационных технологий, связи и защиты информации', type='ORG', start=215) - Entity(text='Департамента информационных технологий , связи и защиты информации', type='ORG', start=215)\n",
      "Entity(text='Омский', type='LOC', start=460) - Entity(text='Омский', type='ORG', start=460)\n",
      "Entity(text='Вильданов', type='PER', start=599) - Entity(text='Вильданов. Родина', type='PER', start=599)\n",
      "Entity(text='Мирослава Немцова', type='PER', start=7705) - Entity(text='Мирослава', type='PER', start=7705)\n",
      "Entity(text='STEM/MARK', type='ORG', start=9016) - Entity(text='STEM / MARK', type='ORG', start=9016)\n",
      "Entity(text='ООО \"Торговый дом \"Шатер\"', type='ORG', start=1068) - Entity(text='ООО \"Торговый дом', type='ORG', start=1068)\n",
      "Entity(text='Военно-морскую академию имени Адмирала Флота Советского Союза Н. Г. Кузнецова', type='ORG', start=783) - Entity(text='Военно-морскую академию имени Адмирала Флота Советского Союза', type='ORG', start=783)\n",
      "Entity(text='Вооруженных Сил', type='ORG', start=898) - Entity(text='Вооруженных Сил. Спиридонов', type='ORG', start=898)\n",
      "Entity(text='В.Высоцкий', type='PER', start=1462) - Entity(text='В.Высоцкий силах', type='PER', start=1462)\n",
      "Entity(text='Молодой гвардии Единой России', type='ORG', start=858) - Entity(text='Молодой гвардии', type='ORG', start=858)\n",
      "Entity(text='Сулеймана Керимова', type='PER', start=1506) - Entity(text='Сулеймана Керимова', type='ORG', start=1506)\n",
      "Entity(text='Министерств внутренних дел', type='ORG', start=164) - Entity(text='Министерств внутренних дел (УВД и МВД)', type='ORG', start=164)\n",
      "Entity(text='Ханты-Мансийскому автономному округу - Югре', type='LOC', start=709) - Entity(text='Ханты-Мансийскому автономному округу-Югре', type='LOC', start=709)\n",
      "Entity(text='МВД', type='ORG', start=1486) - Entity(text='МВД РФ', type='ORG', start=1486)\n",
      "Entity(text='Всероссийского института повышения квалификации сотрудников', type='ORG', start=3126) - Entity(text='Всероссийского института повышения', type='ORG', start=3126)\n",
      "Entity(text='Второго мединститута', type='ORG', start=27) - Entity(text='Второго мединститута. социального развития', type='ORG', start=27)\n",
      "Entity(text='Российского национального исследовательского медицинского университета им.Н.И.Пирогова (Второй мединститут)', type='ORG', start=173) - Entity(text='Российского национального исследовательского медицинского университета им.Н.И.Пирогова', type='ORG', start=173)\n",
      "Entity(text='ОАО \"Современный коммерческий флот\" (ОАО \"Совкомфлот\")', type='ORG', start=206) - Entity(text='ОАО \" Современный коммерческий флот \" (ОАО \" Совкомфлот \")', type='ORG', start=206)\n",
      "Entity(text='ОАО \"Новороссийское морское пароходство\" (ОАО \"Новошип\"', type='ORG', start=616) - Entity(text='ОАО \" Новороссийское морское пароходство \" (ОАО \" Новошип \")', type='ORG', start=616)\n",
      "Entity(text='Севера', type='LOC', start=1484) - Entity(text='Севера', type='ORG', start=1484)\n",
      "Entity(text='Министерство промышленности и энергетики', type='ORG', start=1615) - Entity(text='Министерство промышленности энергетики', type='ORG', start=1615)\n",
      "Entity(text='Дюплесси', type='PER', start=903) - Entity(text='Дюплесси Дюплесси', type='PER', start=903)\n",
      "Entity(text='Департаментом контроля и проверки выполнения решений правительства', type='ORG', start=666) - Entity(text='Департаментом контроля', type='ORG', start=666)\n",
      "Entity(text='Федерального агентства', type='ORG', start=54) - Entity(text='Федерального агентства по управлению государственным имуществом', type='ORG', start=54)\n",
      "Entity(text='Талаат Абдалаа', type='PER', start=285) - Entity(text=\"Талаат Абдалаа-Фараин''\", type='PER', start=285)\n",
      "Entity(text='Сушил Кумара Шинде', type='PER', start=513) - Entity(text='Сушил', type='PER', start=513)\n",
      "Entity(text='ФГУП \"Рособоронэкспорт', type='ORG', start=935) - Entity(text='ФГУП \"Рособоронэкспорт\"', type='ORG', start=935)\n",
      "Entity(text='Самарском областном правительстве', type='ORG', start=1072) - Entity(text='Самарском областном', type='LOC', start=1072)\n",
      "Entity(text='ГУ', type='ORG', start=295) - Entity(text='ГУ МВД', type='ORG', start=295)\n",
      "Entity(text='Организации стран - экспортеров нефти (ОПЕК)', type='ORG', start=627) - Entity(text='Организации стран-экспортеров нефти (ОПЕК)', type='ORG', start=627)\n",
      "Entity(text='Министерства Российской Федерации по делам гражданской обороны, чрезвычайным ситуациям и ликвидации последствий стихийных бедствий', type='ORG', start=542) - Entity(text='Министерства Российской Федерации по делам гражданской обороны , чрезвычайным ситуациям и ликвидации последствий стихийных бедствий', type='ORG', start=542)\n",
      "Entity(text='Министерства Российской Федерации по делам гражданской обороны, чрезвычайным ситуациям и ликвидации последствий стихийных бедствий', type='ORG', start=878) - Entity(text='Министерства Российской Федерации по делам гражданской обороны , чрезвычайным ситуациям и ликвидации последствий стихийных бедствий', type='ORG', start=878)\n",
      "Entity(text='РФ', type='LOC', start=1709) - Entity(text='РФ Силах СССР', type='LOC', start=1709)\n",
      "Entity(text='Ливан', type='LOC', start=237) - Entity(text='Ливан \\r', type='LOC', start=237)\n",
      "Entity(text='Южном округе Нью-Йорка', type='LOC', start=1065) - Entity(text='Южном округе', type='LOC', start=1065)\n",
      "Entity(text='Министерства Российской Федерации по делам гражданской обороны, чрезвычайным ситуациям и ликвидации последствий стихийных бедствий (МЧС)', type='ORG', start=405) - Entity(text='Министерства Российской Федерации по делам гражданской обороны , чрезвычайным ситуациям и ликвидации последствий стихийных бедствий (МЧС)', type='ORG', start=405)\n",
      "Entity(text='ЦАХАЛ', type='ORG', start=36) - Entity(text='ЦАХАЛ \\r', type='ORG', start=36)\n",
      "Entity(text='Габи Ашкенази', type='PER', start=210) - Entity(text='Габи Ашкенази \\r', type='PER', start=210)\n",
      "Entity(text='ОАО \"НК \"Роснефть\"', type='ORG', start=112) - Entity(text='ОАО \" НК \" Роснефть \"', type='ORG', start=112)\n",
      "Entity(text='ОАО \"НК \"Роснефть\"', type='ORG', start=421) - Entity(text='ОАО \" НК \" Роснефть \"', type='ORG', start=421)\n",
      "Entity(text='РФ', type='LOC', start=1078) - Entity(text='РФ Роснефти', type='LOC', start=1078)\n",
      "Entity(text='Европейский союз (ЕС)', type='LOC', start=1864) - Entity(text='Европейский союз (ЕС', type='LOC', start=1864)\n",
      "Entity(text='Северо-Кавказском', type='LOC', start=1295) - Entity(text='Северо-Кавказском федеральном округе', type='LOC', start=1295)\n",
      "Entity(text='Группе компаний \"Sunrise tour\"', type='ORG', start=1740) - Entity(text='Группе компаний \"', type='ORG', start=1740)\n",
      "Entity(text='РЮО', type='LOC', start=1328) - Entity(text='РЮО', type='PER', start=1328)\n",
      "Entity(text='О.Умала', type='PER', start=46) - Entity(text='О.Умала \\r', type='PER', start=46)\n",
      "Entity(text='О.Умалу', type='PER', start=1131) - Entity(text='О.Умалу \\r', type='PER', start=1131)\n",
      "Entity(text='Департамента жилищно-коммунального хозяйства и благоустройства', type='ORG', start=1999) - Entity(text='Департамента жилищно-коммунального хозяйства и', type='ORG', start=1999)\n",
      "Entity(text='Слащева', type='PER', start=0) - Entity(text='Слащева СТС Медиа \"СТС Медиа\"', type='PER', start=0)\n",
      "Entity(text='Юлиана Слащева', type='PER', start=229) - Entity(text='Юлиана Слащева СТС Медиа \"', type='PER', start=229)\n",
      "Entity(text='Моск\\xadве', type='LOC', start=2354) - Entity(text='Моск \\xad ве', type='LOC', start=2354)\n",
      "Entity(text='Дамаск', type='LOC', start=1744) - Entity(text='Дамаск \\n', type='LOC', start=1744)\n",
      "Entity(text='Союзного государства России и Белоруссии', type='LOC', start=177) - Entity(text='Союзного государства', type='LOC', start=177)\n",
      "Entity(text='Буркина-Фасо', type='LOC', start=2) - Entity(text='Буркина-Фасо \\r', type='LOC', start=2)\n",
      "Entity(text='Совете безопасности РФ', type='ORG', start=135) - Entity(text='Совете безопасности', type='ORG', start=135)\n",
      "Entity(text='Москве', type='LOC', start=38) - Entity(text='Москве Дорохова', type='LOC', start=38)\n",
      "Entity(text='Opel/Vauxhall', type='ORG', start=101) - Entity(text='Opel / Vauxhall', type='ORG', start=101)\n",
      "Entity(text='Magna Internationa', type='ORG', start=857) - Entity(text='Magna International', type='ORG', start=857)\n",
      "Entity(text='Главного управления', type='ORG', start=570) - Entity(text='Главного управления внутренних дел', type='ORG', start=570)\n",
      "Entity(text='Главного управления', type='ORG', start=685) - Entity(text='Главного управления внутренних дел', type='ORG', start=685)\n",
      "Entity(text='Расчетная палата ММВБ', type='ORG', start=645) - Entity(text='Расчетная палата', type='ORG', start=645)\n",
      "Entity(text='ЦСКА', type='ORG', start=692) - Entity(text='ЦСКА Сафин', type='ORG', start=692)\n",
      "Entity(text='АФК \"Система\"', type='ORG', start=251) - Entity(text='АФК \"Система\" ', type='ORG', start=251)\n",
      "Entity(text='Народной партией', type='ORG', start=1344) - Entity(text='Народной партией министра', type='ORG', start=1344)\n",
      "Entity(text='Госдумы', type='ORG', start=76) - Entity(text='Госдумы финансовому', type='ORG', start=76)\n",
      "Entity(text='ВТБ', type='ORG', start=800) - Entity(text='ВТБ капитала', type='ORG', start=800)\n",
      "Entity(text='Интерпол', type='ORG', start=940) - Entity(text='Интерпол Гиоргадзе', type='ORG', start=940)\n",
      "Entity(text='Норильского никеля', type='ORG', start=83) - Entity(text='Норильского никеля металлургических компаний', type='ORG', start=83)\n",
      "Entity(text='Андрей Натанович', type='PER', start=715) - Entity(text='Андрей Натанович (Раппопорт', type='PER', start=715)\n",
      "Entity(text='Козака Владимира Васильевича', type='PER', start=389) - Entity(text='Козака', type='PER', start=389)\n",
      "Entity(text='Московский финансовый институт', type='ORG', start=1262) - Entity(text='Московский финансовый институт экономист', type='ORG', start=1262)\n",
      "Entity(text='РАО \"Норильский никель', type='ORG', start=1649) - Entity(text='РАО \"Норильский никель\"', type='ORG', start=1649)\n",
      "Entity(text='Ханты-Мансийского автономного округа - Югры', type='LOC', start=2097) - Entity(text='Ханты-Мансийского автономного округа-Югры', type='LOC', start=2097)\n",
      "Entity(text='Ханты-Мансийского автономного округа - Югры', type='LOC', start=2265) - Entity(text='Ханты-Мансийского автономного округа-Югры', type='LOC', start=2265)\n",
      "Entity(text='СССР', type='LOC', start=535) - Entity(text='СССР Иванова', type='LOC', start=535)\n",
      "Entity(text='Украину', type='LOC', start=944) - Entity(text='Украину RWE', type='LOC', start=944)\n",
      "Entity(text='Гурбангулы Бердымухамедовым', type='PER', start=202) - Entity(text='Гурбангулы Бердымухамедовым Хроника Туркменистана', type='PER', start=202)\n",
      "Entity(text='Главного оперативного управления ГОУ', type='ORG', start=400) - Entity(text='Главного оперативного управления', type='ORG', start=400)\n",
      "Entity(text='А.Силуанов', type='PER', start=1133) - Entity(text='А.Силуанов финансов', type='PER', start=1133)\n",
      "Entity(text='Duracell Batteries N. V', type='ORG', start=1410) - Entity(text='Duracell Batteries N. V. \\r \\n', type='ORG', start=1410)\n",
      "Entity(text='Российская Федераци', type='LOC', start=558) - Entity(text='Российская', type='LOC', start=558)\n",
      "Entity(text='Мурадвердиев', type='PER', start=1207) - Entity(text='Мурадвердиев Алиева', type='PER', start=1207)\n",
      "Entity(text='Ирина Ткачева', type='PER', start=2111) - Entity(text='Ирина', type='PER', start=2111)\n",
      "Entity(text='Самрук-Казына', type='ORG', start=1198) - Entity(text='Самрук-Казына Кулибаев', type='ORG', start=1198)\n",
      "Entity(text='Московский муниципальный банк - Банк Москвы', type='ORG', start=3127) - Entity(text='Московский муниципальный банк-Банк Москвы', type='ORG', start=3127)\n",
      "Entity(text='ОАО \"Акционерный коммерческий банк \"Банк Москвы', type='ORG', start=3264) - Entity(text='ОАО \" Акционерный коммерческий банк \" Банк Москвы \"', type='ORG', start=3264)\n",
      "Entity(text='Евросоюзом (ЕС)', type='LOC', start=864) - Entity(text='Евросоюзом', type='LOC', start=864)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for corrs, preds in zip(y_true['overall'], y_pred['overall']):\n",
    "    for cs, ps in zip(corrs, preds):\n",
    "        for c in corrs:\n",
    "            if ps.start == c.start and c != ps:\n",
    "                print(c, '-', ps)\n",
    "                i += 1\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, не хватило времени разобраться, почему такие получаются такие низкие скоры. В отчете будут приведены полученные здесь и взятые из репозитория Slovnet.\n",
    "\n",
    "UPD: первая модель от Deeppavlov очень странно предсказывало NER-теги (например, Би-би-си оно разбивало как B-ORG E-ORG E-ORG E-ORG E-ORG E-ORG), вторая модель лишена этого недостатка. Остается проблема спорных тегов (например, MEDIA вместо ORG для Spotify). Также в датасете встречаются названия, заключенные или заканчивающиеся различными символами, такими как \" ' ( ) и т. д., притом где-то эти символы входят в тег, где-то - нет, у модели ситуация такая же, поэтому скоры для таких случаев снижаются. Это является (одной из) причиной того, почему EXACT в первом варианте оценки давали околонулевые результаты. Нужно лучше обрабатывать такие случаи или правильно настроить конечные позиции токенов для использовании строки из изначального текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T15:41:47.149313Z",
     "iopub.status.busy": "2023-11-19T15:41:47.149001Z",
     "iopub.status.idle": "2023-11-19T15:41:47.820841Z",
     "shell.execute_reply": "2023-11-19T15:41:47.819771Z",
     "shell.execute_reply.started": "2023-11-19T15:41:47.149274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL F1 для Deeppavlov BERT: 0.9556805574478854\n",
      "LOC F1 для Deeppavlov BERT: 0.9877088799889518\n",
      "PER F1 для Deeppavlov BERT: 0.9925225733634312\n",
      "ORG F1 для Deeppavlov BERT: 0.873776469835583\n"
     ]
    }
   ],
   "source": [
    "for metric in y_true.keys():\n",
    "    print(f\"{metric.upper()} F1 для Deeppavlov BERT: {nereval.evaluate(y_true[metric], y_pred[metric])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:11:53.671759Z",
     "iopub.status.busy": "2023-11-18T17:11:53.670935Z",
     "iopub.status.idle": "2023-11-18T17:13:25.307707Z",
     "shell.execute_reply": "2023-11-18T17:13:25.306756Z",
     "shell.execute_reply.started": "2023-11-18T17:11:53.671721Z"
    }
   },
   "outputs": [],
   "source": [
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "wandb.init(project=\"Testing NER models\")\n",
    "for tagged_text in ne5:\n",
    "    res = slovnet(tagged_text.text)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:39:34.003577Z",
     "iopub.status.busy": "2023-11-18T17:39:34.002803Z",
     "iopub.status.idle": "2023-11-18T17:40:49.941955Z",
     "shell.execute_reply": "2023-11-18T17:40:49.941213Z",
     "shell.execute_reply.started": "2023-11-18T17:39:34.003540Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "ne5 = corus.load_ne5('/kaggle/working/Collection5')\n",
    "wandb.init(project=\"Testing NER models\")\n",
    "for tagged_text in ne5:\n",
    "    cur_text = tagged_text.text\n",
    "    batch_size=512\n",
    "    splitted_text = re.split(r'(\\s+)', cur_text)\n",
    "    token_batches = [splitted_text[i:i + batch_size] for i in range(0, len(splitted_text), batch_size)]\n",
    "    for batch in token_batches:\n",
    "        input_text = ''.join(batch)\n",
    "        tokens, ner_tokens, offsets = ner_model([input_text])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4013944,
     "sourceId": 6984206,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
